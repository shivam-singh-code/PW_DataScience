{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNH7KD0IaA3P6Y44fo8j8Mk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Feature Engineering**"],"metadata":{"id":"kch6iQI-frEC"}},{"cell_type":"markdown","source":["1. **What is a parameter?**\n","\n","    A **parameter** is a **variable that the learning algorithm learns automatically from the training data**.  \n","    These values define how a model makes predictions and are adjusted during the **training process** to minimize error or loss.\n","\n","    In simple terms:  \n","    > **Parameters are the internal configuration values of a model that are learned during training.**\n","\n","    **Examples**\n","\n","    | Model Type | Example Parameters | Description |\n","    |-------------|-------------------|--------------|\n","    | **Linear Regression** | Weights (β₁, β₂, …) and intercept (β₀) | Define the slope and position of the regression line |\n","    | **Logistic Regression** | Coefficients and bias | Define how input features affect the probability of a class |\n","    | **Neural Networks** | Weights and biases of neurons | Adjusted during backpropagation to minimize loss |\n","    | **Decision Trees** | Split thresholds | Determine how data is divided at each node |\n","\n","\n","    **Example (Linear Regression)**\n","\n","    For a simple linear model:\n","\n","    $$\n","    y = β_0 + β_1x + ε\n","    $$\n","\n","    - $ β_0 $ and $ β_1 $ are **parameters** learned from the data.  \n","    - $ ε $ is the **error term** (not a parameter, just noise)."],"metadata":{"id":"_JmlVb3MgNBX"}},{"cell_type":"markdown","metadata":{"id":"8ba6defd"},"source":["2. **What is correlation? What does negative correlation mean?**\n","   \n","   **Correlation** is a statistical measure that describes the **strength and direction of a relationship between two variables**.\n","   \n","   It tells us **how changes in one variable are associated with changes in another**.\n","   \n","   The correlation coefficient (usually denoted as **r**) ranges from **-1 to +1**\n","   \n","   **Types of Correlation**\n","   \n","  | Type | Range of r | Meaning | Example |\n","  |------|-------------|----------|----------|\n","  | **Positive Correlation** | `0 < r ≤ +1` | As one variable increases, the other also increases. | Height vs. Weight |\n","  | **Negative Correlation** | `-1 ≤ r < 0` | As one variable increases, the other decreases. | Price vs. Demand |\n","  | **No Correlation** | `r ≈ 0` | No linear relationship between variables. | Shoe size vs. IQ |\n","   \n","   **What Does Negative Correlation Mean?**\n","   \n","   A **negative correlation** means that:\n","   > When one variable increases, the other tends to decrease.\n","\n","   \n","   It represents an **inverse relationship**.\n","   \n","   **Example:**\n","   - As the **price** of a product increases, the **demand** usually decreases.\\\n","→ This is a **negative correlation**.\n","   \n","   If we calculate and get `r = -0.85`, it means:\n","   - The relationship is **strong** (since |r| is close to 1)\n","   - The direction is **negative** (one goes up, the other goes down)\n","   \n","   * * *\n","   \n","   **Formula (Pearson Correlation Coefficient)**\n","   \n","   $$\n","r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\n","$$\n","   \n","   Where:\n","   - $ x_i, y_i $ → individual data points  \n","   - $ \\bar{x}, \\bar{y} $ → mean of each variable"]},{"cell_type":"markdown","source":["3. **Define Machine Learning. What are the main components in Machine Learning?**\n","\n","\n","  **Machine Learning (ML)** is a branch of **Artificial Intelligence (AI)** that enables systems to **learn automatically from data** and **improve their performance** without being explicitly programmed.\n","\n","  In simple terms:\n","\n","  > Machine Learning is about teaching computers to **find patterns** in data and **make decisions or predictions** based on those patterns.\n","\n","  ---\n","\n","  **Example**\n","\n","  When you use:\n","  - **Netflix** → recommends movies you might like  \n","  - **Gmail** → filters spam emails  \n","  - **Amazon** → suggests products based on your browsing history  \n","\n","  All these systems use **machine learning models** that have **learned** from past data.\n","\n","  ---\n","\n","  **Main Components of Machine Learning**\n","\n","  Machine Learning systems are built using several key components:\n","\n","  1. **Data**\n","  - The **foundation** of machine learning.\n","  - Represents past observations, measurements, or examples.\n","  - Can be:\n","    - **Labeled data** (used in **supervised learning**)\n","    - **Unlabeled data** (used in **unsupervised learning**)\n","    - **Mixed data** (used in **semi-supervised learning**)\n","\n","  Example: A dataset of customer purchases, medical records, or images.\n","\n","  ---\n","\n","  2. **Model**\n","  - The **mathematical representation** that learns patterns from data.\n","  - The model tries to **map inputs to outputs**.\n","\n","  Example:\n","  - Linear Regression model: $ y = w_1x + b $\n","  - Neural Network model: multiple layers of weights and biases.\n","\n","  ---\n","\n","  3. **Parameters**\n","  - **Internal variables** of the model that are learned from training data.\n","  - Adjusted to minimize prediction error.\n","\n","  Example:\n","  - In linear regression: slope (**w₁**) and intercept (**b**) are parameters.\n","\n","  ---\n","\n","  4. **Learning (or Training) Algorithm**\n","  - The process that updates model parameters using training data.\n","  - The goal is to **reduce the loss/error function**.\n","\n","  Example:\n","  - Gradient Descent — adjusts weights step-by-step to minimize loss.\n","\n","  ---\n","\n","  5. **Loss Function (or Cost Function)**\n","  - Measures how well or poorly the model performs.\n","  - Quantifies the difference between **predicted output** and **actual output**.\n","\n","  Example:\n","  $$\n","  \\text{MSE (Mean Squared Error)} = \\frac{1}{n} \\sum (y_{pred} - y_{true})^2\n","  $$\n","\n","  ---\n","\n","  6. **Evaluation Metrics**\n","  - Used to assess the model’s performance on unseen (test) data.\n","\n","  Example:\n","  - Accuracy, Precision, Recall, F1-Score, RMSE, etc.\n","\n","  ---\n","\n","  7. **Prediction (or Inference)**\n","  - The final step where the trained model is used to make predictions on new, unseen data.\n","\n","  Example:\n","  - Predicting house prices based on size and location.\n","\n","  ---\n","\n","\n","  | Component | Description | Example |\n","  |------------|--------------|----------|\n","  | **Data** | Information used for learning | Customer purchase records |\n","  | **Model** | Mathematical structure | Linear regression line |\n","  | **Parameters** | Learnable weights | Coefficients in regression |\n","  | **Learning Algorithm** | Method to update parameters | Gradient descent |\n","  | **Loss Function** | Measures prediction error | Mean squared error |\n","  | **Evaluation Metrics** | Performance indicators | Accuracy, F1-score |\n","  | **Prediction** | Output on new data | Predicting future sales |"],"metadata":{"id":"nPZDF1-fjJlp"}},{"cell_type":"markdown","source":["4. **How does loss value help in determining whether the model is good or not?**\n","\n","In Machine Learning, the **loss value** (also called **cost** or **error**) is a numerical measure of **how well or poorly a model’s predictions match the actual data**.\n","\n","It represents the **difference between the predicted values and the true values** from the training data.\n","\n","A **smaller loss value** indicates that the model’s predictions are closer to the true outputs, while a **larger loss value** means the model is making bigger errors.\n","\n","---\n","\n","**Purpose of the Loss Function**\n","\n","The **loss function** helps guide the learning process of a model.  \n","During training, the learning algorithm (like gradient descent) tries to **minimize this loss** by adjusting the model’s parameters (weights and biases).\n","\n","This process continues until the loss value stops decreasing or reaches a small enough value, indicating that the model has learned the data patterns well.\n","\n","---\n","\n","**Example**\n","\n","For a regression problem, one common loss function is the **Mean Squared Error (MSE):**\n","\n","$$\n","\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{pred},i} - y_{\\text{true},i})^2\n","$$\n","\n","- $ y_{\\text{pred},i} $: predicted value  \n","- $ y_{\\text{true},i} $: actual value  \n","- $ n $: number of data points\n","\n","A lower MSE means predictions are closer to the actual values.\n","\n","---\n","\n","**How the Loss Value Indicates Model Quality**\n","\n","| Loss Value | Interpretation |\n","|-------------|----------------|\n","| **High loss** | The model is performing poorly; predictions are far from the true values. |\n","| **Low loss** | The model is performing well; predictions are close to the true values. |\n","| **Constant or increasing loss** | The model may not be learning, possibly due to poor model design, wrong learning rate, or noisy data. |\n","| **Very low loss but poor test accuracy** | The model may be overfitting — it has learned the training data too well but generalizes poorly. |\n","\n","---\n","\n","## Example in Practice\n","\n","Suppose a regression model has:\n","- **Training loss = 0.02**\n","- **Validation loss = 0.03**\n","\n","This indicates the model performs well both on training and unseen data.\n","\n","However, if:\n","- **Training loss = 0.01**\n","- **Validation loss = 0.25**\n","\n","Then the model likely **overfits**, meaning it performs well on training data but poorly on new data."],"metadata":{"id":"DMct73vnkLLa"}},{"cell_type":"markdown","source":["5. **What are continuous and categorical variables?**\n","\n","In Machine Learning and Statistics, variables represent the features or attributes of data that we analyze or use to train models.  \n","They can broadly be classified into two main types: **Continuous** and **Categorical** variables.\n","\n","---\n","\n","1. Continuous Variables:\n","A **continuous variable** is a numerical variable that can take **any value within a given range**.  \n","The values are **measurable** and can include fractions or decimals.\n","\n","Examples:\n","- Height of a person (e.g., 170.5 cm)\n","- Weight of an object (e.g., 65.3 kg)\n","- Temperature (e.g., 36.6°C)\n","- Time taken to complete a task (e.g., 12.45 seconds)\n","\n","Characteristics:\n","- Infinite possible values within a range.  \n","- Represented using **real numbers**.  \n","- Operations like addition, subtraction, and averaging are meaningful.  \n","\n","Example in Data:\n","| Person | Height (cm) | Weight (kg) |\n","|---------|--------------|-------------|\n","| A | 172.5 | 68.4 |\n","| B | 160.2 | 55.7 |\n","\n","---\n","\n","2. Categorical Variables:\n","A **categorical variable** (also called **qualitative variable**) represents data that can be **divided into categories or groups**.  \n","The values are **labels or names**, not numbers with mathematical meaning.\n","\n","Types of Categorical Variables:\n","1. **Nominal Variables** – Categories with **no inherent order**  \n","   - Examples: Gender (Male/Female), Color (Red/Blue/Green), Country (India/USA/UK)\n","2. **Ordinal Variables** – Categories with a **specific order or ranking**  \n","   - Examples: Education Level (High School < Bachelor’s < Master’s < PhD), Customer Satisfaction (Low < Medium < High)\n","\n","Example in Data:\n","| Student | Gender | Education Level |\n","|----------|---------|----------------|\n","| X | Male | Bachelor’s |\n","| Y | Female | Master’s |"],"metadata":{"id":"Mo95qVkqlC0a"}},{"cell_type":"markdown","source":["6. **How do we handle categorical variables in Machine Learning? What are the common techniques?**\n","\n","Categorical variables represent discrete values or categories, such as \"red\", \"blue\", \"green\", or \"male\", \"female\". Most machine learning algorithms require numerical input, so categorical variables must be converted into a numerical format.  \n","\n","**Common Techniques:**  \n","\n","**1. Label Encoding**  \n","- Assigns a unique integer to each category.  \n","- Example: `Red → 0, Blue → 1, Green → 2`  \n","- Suitable for ordinal variables where the order matters (e.g., `Low → 0, Medium → 1, High → 2`).  \n","- **Caution:** For nominal data, this may introduce an unintended ordinal relationship.  \n","\n","**2. One-Hot Encoding**  \n","- Converts each category into a new binary column (0 or 1).  \n","- Example for color:  \n","\n","| Red | Blue | Green |\n","|-----|------|-------|\n","| 1   | 0    | 0     |\n","| 0   | 1    | 0     |\n","| 0   | 0    | 1     |\n","\n","- Commonly used for nominal variables.  \n","- Increases dimensionality for high-cardinality features.  \n","\n","**3. Ordinal Encoding**  \n","- Similar to label encoding but preserves meaningful order in categories.  \n","- Example:\n","\n","| Size  | Encoded |\n","|-------|---------|\n","| Small | 1       |\n","| Medium| 2       |\n","| Large | 3       |\n","\n","- Useful for features with natural ordering.  \n","\n","**4. Binary Encoding**  \n","- Converts categories into binary numbers and splits them into separate columns.  \n","- Example for colors (`Red=0, Blue=1, Green=2`) in binary:  \n","\n","| Red | Blue | Green |\n","|-----|------|-------|\n","| 00  | 01   | 10    |\n","\n","- Reduces dimensionality compared to one-hot encoding for high-cardinality features.  \n","\n","**5. Frequency / Count Encoding**  \n","- Replaces each category with its frequency or count in the dataset.  \n","- Example:  \n","\n","| Color | Frequency |\n","|-------|-----------|\n","| Red   | 50        |\n","| Blue  | 30        |\n","| Green | 20        |\n","\n","- Useful for tree-based algorithms.  \n","\n","**6. Target Encoding (Mean Encoding)**  \n","- Replaces categories with the mean of the target variable for each category.  \n","- Example (predicting sales):  \n","\n","| Color | Average Sales |\n","|-------|---------------|\n","| Red   | 250           |\n","| Blue  | 180           |\n","| Green | 200           |\n","\n","- Powerful but prone to overfitting; usually requires cross-validation or smoothing.  \n","\n","**7. Embedding Layers (for Deep Learning)**  \n","- Represent categories as dense vectors in a lower-dimensional space.  \n","- Example:  \n","\n","| Color | Embedding Vector      |\n","|-------|---------------------|\n","| Red   | [0.1, 0.3, 0.7]     |\n","| Blue  | [0.2, 0.6, 0.4]     |\n","| Green | [0.9, 0.1, 0.5]     |\n","\n","- Learns relationships between categories during training.  \n","\n","**8. Hashing Encoding**  \n","- Maps categories to fixed-size hash buckets.  \n","- Example (3 buckets):  \n","\n","| Color | Hash Bucket |\n","|-------|-------------|\n","| Red   | 0           |\n","| Blue  | 1           |\n","| Green | 2           |\n"],"metadata":{"id":"KgcYIpmOlnw8"}},{"cell_type":"markdown","source":["7. **What do you mean by training and testing a dataset?**\n","\n","In Machine Learning, we use datasets to teach the model patterns and then evaluate its performance. To do this effectively, we split the data into two main parts: **training** and **testing**.  \n","\n","**1. Training a Dataset**  \n","- **Definition:** Training a dataset means using a portion of your data to **teach the model** the relationships, patterns, and features in the data.  \n","- During training, the model learns from the input data (features) and the correct outputs (labels) to minimize errors.  \n","- Example: If you are predicting house prices, the model sees features like size, location, and age, and learns to predict the price.  \n","- The process involves adjusting the model’s parameters (weights in neural networks) based on the training data.  \n","\n","**2. Testing a Dataset**  \n","- **Definition:** Testing a dataset means using a separate portion of your data to **evaluate how well the model performs** on unseen data.  \n","- The testing data is **not shown to the model during training**, which helps check if the model can generalize beyond what it learned.  \n","- Example: You give the model new house features and check how close its predicted prices are to the actual prices.  \n","\n","**Why We Split Data**  \n","- Prevent **overfitting:** If you train and test on the same data, the model may memorize the training examples but fail on new data.  \n","- Evaluate **generalization:** Testing ensures the model works well on unseen, real-world data.  \n","\n","**Typical Split Ratios**  \n","- 70% training, 30% testing (common)  \n","- 80% training, 20% testing (for larger datasets)  \n","- Sometimes, a third **validation set** is used to fine-tune hyperparameters:  \n","  - 60% training, 20% validation, 20% testing  \n","  "],"metadata":{"id":"rY_gMr3LpOsl"}},{"cell_type":"markdown","source":["8. **What is sklearn.preprocessing?**\n","\n","sklearn.preprocessing is a **module in the Scikit-learn library** in Python that provides **tools to prepare your data** before feeding it into a machine learning model. Preprocessing helps improve model performance, speed up training, and ensure features are on comparable scales.  \n","\n","**Key Purposes of sklearn.preprocessing:**  \n","- Scale numerical features  \n","- Encode categorical features  \n","- Handle missing values (partially via transformers)  \n","- Transform features into a format suitable for machine learning algorithms  \n","\n","**Common Classes and Functions in sklearn.preprocessing:**  \n","\n","**1. StandardScaler**  \n","- Standardizes features by removing the mean and scaling to unit variance.  \n","- Formula: `z = (x - mean) / std`  \n","- Useful for algorithms like SVM, KNN, or logistic regression.  \n","\n","**2. MinMaxScaler**  \n","- Scales features to a fixed range, usually [0, 1].  \n","- Formula: `x_scaled = (x - min) / (max - min)`  \n","- Useful when features have different units.  \n","\n","**3. RobustScaler**  \n","- Scales features using the median and interquartile range (IQR).  \n","- Less sensitive to outliers compared to StandardScaler.  \n","\n","**4. Normalizer**  \n","- Scales individual samples to have unit norm (length = 1).  \n","- Often used for text classification or clustering.  \n","\n","**5. OneHotEncoder**  \n","- Converts categorical variables into a one-hot numeric array.  \n","- Example:  \n","\n","| Color | Red | Blue | Green |\n","|-------|-----|------|-------|\n","| Red   | 1   | 0    | 0     |\n","| Blue  | 0   | 1    | 0     |\n","| Green | 0   | 0    | 1     |\n","\n","**6. LabelEncoder**  \n","- Converts categorical labels into numeric form.  \n","- Example: `Red → 0, Blue → 1, Green → 2`  \n","\n","**7. PolynomialFeatures**  \n","- Generates polynomial and interaction features from existing features.  \n","- Useful for linear models to capture non-linear relationships.  \n","\n","**8. FunctionTransformer**  \n","- Apply a custom function to transform your data.  \n","\n","**Summary:**  \n","`sklearn.preprocessing` is essential for preparing your dataset—scaling, normalizing, encoding, or transforming—so your machine learning models work efficiently and accurately.\n"],"metadata":{"id":"z3WNQGk9qFEK"}},{"cell_type":"markdown","source":["9. **What is a Test set?**\n","\n","A **test set** is a **subset of your dataset** that is used to **evaluate the performance of a trained machine learning model**. It is **separate from the training set**, which is used to teach the model.  \n","\n","**Key Points about a Test Set:**  \n","\n","**1. Purpose**  \n","- To measure how well the model **generalizes** to new, unseen data.  \n","- Helps detect **overfitting**, where the model performs well on training data but poorly on unseen data.  \n","\n","**2. How it is Created**  \n","- Typically, the dataset is split into:  \n","  - **Training set:** 70–80% of the data  \n","  - **Test set:** 20–30% of the data  \n","  - Optionally, a **validation set** can be used for hyperparameter tuning.  \n","\n","**3. Characteristics**  \n","- The test set **must not be used during training**.  \n","- Should be **representative of real-world data** to ensure meaningful evaluation.  \n","\n","**4. Evaluation Metrics**  \n","- For regression: Mean Squared Error (MSE), R² score  \n","- For classification: Accuracy, Precision, Recall, F1-score  \n","\n","**5. Example**  \n","\n","Suppose we have 1000 data points:  \n","\n","| Dataset Split | Number of Samples |\n","|---------------|-----------------|\n","| Training Set  | 800             |\n","| Test Set      | 200             |\n","\n","- Model learns patterns from the **training set (800 samples)**.  \n","- Model is then tested on the **test set (200 samples)** to check performance on unseen data."],"metadata":{"id":"IAYBZgqWqTua"}},{"cell_type":"markdown","source":["10. **How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?**\n","\n","In Machine Learning, we **split the dataset** into a **training set** (to train the model) and a **test set** (to evaluate the model). This is commonly done using `train_test_split` from `sklearn.model_selection`.  \n","\n","**1. Using train_test_split**  \n","\n","```python\n","from sklearn.model_selection import train_test_split\n","\n","# Example dataset\n","X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]  # Features\n","y = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]          # Labels\n","\n","# Split into 70% training and 30% testing\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","print(\"Training Features:\", X_train)\n","print(\"Testing Features:\", X_test)\n","```\n","**Parameters:**\n","*test_size: Proportion of the dataset to include in the test split (e.g., 0.2 = 20%)\n","* train_size: Optional, proportion for the training set\n","* random_state: Ensures reproducibility\n","* shuffle: Whether to shuffle the data before splitting (default is True)\n","\n","**Approaching a Machine Learning Problem**\n","1. Define the Problem\n","- Understand the goal (e.g., classification, regression, clustering)\n","- Identify inputs (features) and outputs (target)\n","\n","2. Collect and Explore Data\n","- Gather dataset from sources\n","- Perform Exploratory Data Analysis (EDA):\n","    - Check distributions, missing values, outliers\n","    - Visualize relationships between features and target\n","\n","3. Preprocess Data\n","- Handle missing values\n","- Encode categorical variables (LabelEncoder, OneHotEncoder)\n","- Scale or normalize features (StandardScaler, MinMaxScaler)\n","\n","4. Split Data\n","- Divide dataset into training and testing sets (and optionally validation set)\n","\n","5. Choose a Model\n","- Select a model based on the problem:\n","    - Regression: Linear Regression, Random Forest, XGBoost\n","    - Classification: Logistic Regression, SVM, Decision Tree\n","    - Clustering: K-Means, DBSCAN\n","\n","6. Train the Model\n","- Fit the model using the training set\n","\n","7. Evaluate the Model\n","- Use the test set to measure performance\n","- Metrics examples:\n","    - Regression: Mean Squared Error, R² score\n","    - Classification: Accuracy, Precision, Recall, F1-score\n","\n","8. Tune Hyperparameters\n","- Use cross-validation or grid search (GridSearchCV) to improve performance\n","\n","9. Deploy or Interpret Results\n","- Apply model to new data or integrate into applications\n","- Interpret feature importance or model behavior if needed"],"metadata":{"id":"Z-STB2Ojqbx-"}},{"cell_type":"markdown","source":["11. **Why do we have to perform EDA before fitting a model to the data?**\n","\n","**Why Perform Exploratory Data Analysis (EDA) Before Fitting a Model**  \n","\n","Exploratory Data Analysis (EDA) is a critical step in any machine learning workflow. It involves examining and visualizing your dataset to understand its main characteristics before building a model.  \n","\n","**Reasons for Performing EDA:**  \n","\n","**1. Understanding Data Distribution**  \n","- Helps identify how features are distributed (normal, skewed, uniform, etc.).  \n","- Important for choosing the right algorithms and preprocessing steps.  \n","\n","**2. Detecting Missing Values**  \n","- Missing or null values can cause errors during model training.  \n","- EDA helps identify missing data and decide whether to impute, drop, or fill values.  \n","\n","**3. Identifying Outliers**  \n","- Outliers can distort model predictions, especially in regression.  \n","- EDA visualizations (boxplots, scatterplots) help detect outliers and handle them appropriately.  \n","\n","**4. Understanding Feature Relationships**  \n","- Correlation analysis shows relationships between features and with the target variable.  \n","- Helps in feature selection and avoiding multicollinearity.  \n","\n","**5. Detecting Data Imbalances**  \n","- In classification problems, EDA helps check if classes are imbalanced.  \n","- Imbalanced data may require resampling techniques like SMOTE or class weighting.  \n","\n","**6. Guiding Preprocessing Steps**  \n","- Scaling, normalization, encoding, and feature engineering decisions are informed by EDA.  \n","\n","**7. Informing Model Choice**  \n","- Based on data type, distribution, and relationships, you can choose models likely to perform well.  \n","\n","**Note**\n","EDA ensures you **understand your data** thoroughly, catch potential issues early, and make informed decisions about preprocessing and model selection. Skipping EDA may lead to poor model performance, overfitting, or misleading results.  \n"],"metadata":{"id":"K5DGSYS7razr"}},{"cell_type":"markdown","source":["12. **What is correlation?**\n","\n","**Correlation in Statistics and Machine Learning**  \n","\n","**Definition:**  \n","Correlation is a statistical measure that describes the **strength and direction of a linear relationship** between two variables. It quantifies how changes in one variable are associated with changes in another.  \n","\n","**Types of Correlation:**  \n","\n","**1. Positive Correlation**  \n","- When one variable increases, the other also increases.  \n","- Example: Height and weight — generally, taller people weigh more.  \n","- Correlation coefficient (r) is between 0 and +1.  \n","\n","**2. Negative Correlation**  \n","- When one variable increases, the other decreases.  \n","- Example: Number of hours spent watching TV and exam scores — more TV, lower scores.  \n","- Correlation coefficient (r) is between -1 and 0.  \n","\n","**3. No Correlation**  \n","- No linear relationship exists between the variables.  \n","- Example: Shoe size and intelligence — usually independent.  \n","- Correlation coefficient (r) is around 0.  \n","\n","**Correlation Coefficient (r):**  \n","- Quantifies correlation: ranges from **-1 to +1**.  \n","  - `r = 1` → perfect positive correlation  \n","  - `r = -1` → perfect negative correlation  \n","  - `r = 0` → no linear correlation  \n","\n","**Why Correlation is Important:**  \n","- Helps **identify relationships** between features.  \n","- Useful for **feature selection** in machine learning (e.g., remove highly correlated features to avoid multicollinearity).  \n","- Guides understanding of **data patterns** before modeling.  "],"metadata":{"id":"QdwcQnOArm8b"}},{"cell_type":"markdown","source":["13. **What does negative correlation mean?**\n","\n","**Negative Correlation**  \n","\n","**Definition:**  \n","Negative correlation occurs when **one variable increases while the other decreases**. It represents an **inverse relationship** between two variables.  \n","\n","**Key Points:**  \n","- The correlation coefficient (r) is **between -1 and 0**.  \n","  - `r = -1` → perfect negative correlation (exact inverse relationship)  \n","  - `r = 0` → no linear correlation  \n","- As one variable goes up, the other tends to go down.  \n","\n","**Examples:**  \n","1. **Number of hours spent watching TV vs. exam scores**  \n","   - More TV → lower scores → negative correlation.  \n","2. **Temperature vs. heating bill**  \n","   - Higher temperature → lower heating bill → negative correlation.  \n","\n","**Why It Matters in Machine Learning:**  \n","- Helps identify features that are inversely related to the target.  \n","- Can guide feature selection or transformation.  \n","- Important for understanding relationships between variables before modeling.  \n","\n","**Visual Representation:**  \n","\n","| Variable X | Variable Y |\n","|------------|------------|\n","| 1          | 10         |\n","| 2          | 8          |\n","| 3          | 6          |\n","| 4          | 4          |\n","| 5          | 2          |\n","\n","- As X increases, Y decreases → negative correlation.  \n"],"metadata":{"id":"OprWcrPCr0Ob"}},{"cell_type":"markdown","source":["14. **How can you find correlation between variables in Python**\n","\n","In Python, you can calculate correlation between variables using **Pandas** or **NumPy**. The most common method is the **Pearson correlation coefficient**, which measures linear relationships.  \n","\n","**1. Using Pandas `corr()` Method**\n","** 2. Using NumPy corrcoef() Function**\n"],"metadata":{"id":"VLhC9kS9r95j"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Example dataset\n","data = {\n","    'Hours_Studied': [2, 4, 6, 8, 10],\n","    'Scores': [50, 60, 65, 80, 90],\n","    'Hours_TV': [10, 8, 6, 4, 2]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Correlation between all variables\n","correlation_matrix = df.corr()\n","print(correlation_matrix)\n","\n","# Correlation between two specific variables\n","corr_hours_scores = df['Hours_Studied'].corr(df['Scores'])\n","print(\"Correlation between Hours_Studied and Scores:\", corr_hours_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L54RI-IZsNjT","executionInfo":{"status":"ok","timestamp":1761132733389,"user_tz":-330,"elapsed":749,"user":{"displayName":"shivam singh","userId":"13846532010104603200"}},"outputId":"9e43713d-c330-49c4-d78e-c3fc366b5cff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["               Hours_Studied    Scores  Hours_TV\n","Hours_Studied       1.000000  0.990148 -1.000000\n","Scores              0.990148  1.000000 -0.990148\n","Hours_TV           -1.000000 -0.990148  1.000000\n","Correlation between Hours_Studied and Scores: 0.9901475429766743\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","x = np.array([2, 4, 6, 8, 10])\n","y = np.array([50, 60, 65, 80, 90])\n","\n","# Calculate correlation coefficient matrix\n","corr_matrix = np.corrcoef(x, y)\n","print(corr_matrix)\n","\n","# The correlation coefficient between x and y\n","corr_xy = corr_matrix[0, 1]\n","print(\"Correlation between x and y:\", corr_xy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0Rz921osN-w","executionInfo":{"status":"ok","timestamp":1761132766800,"user_tz":-330,"elapsed":28,"user":{"displayName":"shivam singh","userId":"13846532010104603200"}},"outputId":"99b70dbd-41a5-4976-a34b-28fc4a255211"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1.         0.99014754]\n"," [0.99014754 1.        ]]\n","Correlation between x and y: 0.9901475429766743\n"]}]},{"cell_type":"markdown","source":["15. **What is causation? Explain difference between correlation and causation with an example.**\n","\n","Causation (or causal relationship) occurs when **a change in one variable directly causes a change in another variable**. It implies a **cause-and-effect relationship**.  \n","\n","**Key Points of Causation:**  \n","- Causation is **stronger than correlation** because it shows that one event actually influences another.  \n","- Establishing causation usually requires controlled experiments or advanced statistical methods, not just observational data.  \n","\n","---\n","\n","**Correlation vs. Causation**  \n","\n","| Aspect             | Correlation                                   | Causation                                   |\n","|-------------------|-----------------------------------------------|--------------------------------------------|\n","| Definition        | Measures the strength and direction of a relationship between two variables | One variable directly affects or causes a change in another |\n","| Nature            | Statistical association, can be positive, negative, or zero | Cause-and-effect relationship |\n","| Evidence Needed   | Observational data or statistical computation | Controlled experiments, intervention, or causal inference techniques |\n","| Interpretation    | Does not imply one variable causes the other | Implies direct impact of one variable on another |\n","\n","**Example:**  \n","- **Correlation Example:**  \n","  - Ice cream sales and drowning incidents may be positively correlated (both increase in summer).  \n","  - But buying ice cream **does not cause** drowning.  \n","- **Causation Example:**  \n","  - Smoking **causes** lung cancer. Scientific studies have shown a direct cause-effect relationship.  \n","\n","**Key Takeaway:**  \n","- **Correlation ≠ Causation.**  \n","- Correlation tells you variables move together; causation tells you one variable **directly influences** the other.  \n","- Confusing correlation with causation can lead to incorrect conclusions and poor decision-making.  \n","\n"],"metadata":{"id":"L_61ySt9se6j"}},{"cell_type":"markdown","source":["16. **What is an Optimizer? What are different types of optimizers? Explain each with an example**\n","\n","An **optimizer** is an algorithm used to **update the parameters (weights and biases) of a machine learning model** during training in order to **minimize the loss function**.  \n","\n","**Purpose:**  \n","- The optimizer helps the model **learn from data** by adjusting its parameters to reduce errors.  \n","- It determines **how quickly and in which direction** the weights are updated.  \n","\n","**How it Works:**  \n","1. The model makes predictions using current weights.  \n","2. The **loss function** calculates the error between predictions and actual values.  \n","3. The optimizer updates the weights in the **direction that minimizes the loss**.  \n","4. This process repeats over multiple iterations (epochs) until the model converges.  \n","\n","**Example in Python using SGD (Stochastic Gradient Descent):**  \n","\n","```python\n","from tensorflow.keras.optimizers import SGD\n","\n","# Create an SGD optimizer with learning rate 0.01\n","optimizer = SGD(learning_rate=0.01)\n","```\n","\n","**Different Types of Optimizers in Machine Learning**  \n","\n","1. **SGD (Stochastic Gradient Descent)**  \n","   - Updates weights using one sample or mini-batch at a time.  \n","   - Simple and widely used.  \n","   ```python\n","    from tensorflow.keras.optimizers import SGD\n","    optimizer = SGD(learning_rate=0.01)\n","    ```\n","\n","2. **Momentum**  \n","   - Adds a fraction of the previous update to the current update to accelerate convergence and reduce oscillations.\n","   ```python\n","   from tensorflow.keras.optimizers import SGD\n","\n","   optimizer = SGD(learning_rate=0.01, momentum=0.9)\n","   ```\n","3. **Adagrad (Adaptive Gradient Algorithm)**  \n","   - Adjusts learning rate for each parameter individually based on update frequency; good for sparse data.\n","   ```python\n","   from tensorflow.keras.optimizers import Adagrad\n","   optimizer = Adagrad(learning_rate=0.01)\n","   ```\n","4. **RMSProp (Root Mean Square Propagation)**  \n","   - Adaptive learning rate with decay; handles non-stationary objectives well (e.g., RNNs).\n","   ```python\n","   from tensorflow.keras.optimizers import RMSprop\n","   optimizer = RMSprop(learning_rate=0.001, rho=0.9)\n","   ```\n","\n","5. **Adam (Adaptive Moment Estimation)**  \n","   - Combines momentum and RMSProp; tracks first and second moments of gradients; fast and stable.  \n","   ```python\n","   from tensorflow.keras.optimizers import Adam\n","   optimizer = Adam(learning_rate=0.001)\n","   ```\n","\n","6. **Nadam (Nesterov-accelerated Adam)**  \n","   - Adam optimizer with Nesterov momentum for slightly faster convergence.  \n","   ```python\n","   from tensorflow.keras.optimizers import Nadam\n","   optimizer = Nadam(learning_rate=0.001)\n","   ```\n"],"metadata":{"id":"MWTaFBNas5Gc"}},{"cell_type":"markdown","source":["17. **What is sklearn.linear_model?**\n","\n","**`sklearn.linear_model` in Python**  \n","\n","**Definition:**  \n","`sklearn.linear_model` is a module in **scikit-learn** that contains **linear models** for regression and classification.  \n","Linear models assume a **linear relationship** between input features and the target variable.  \n","\n","**Common Models in `sklearn.linear_model`:**  \n","\n","1. **LinearRegression**  \n","   - Predicts a continuous target using a linear combination of input features.  \n","   - Example: Predicting house prices.  \n","\n","2. **LogisticRegression**  \n","   - Used for binary or multiclass classification.  \n","   - Outputs probabilities and predicts classes using a sigmoid or softmax function.  \n","\n","3. **Ridge Regression**  \n","   - Linear regression with **L2 regularization** to prevent overfitting.  \n","\n","4. **Lasso Regression**  \n","   - Linear regression with **L1 regularization**, can shrink some coefficients to zero (feature selection).  \n","\n","5. **ElasticNet**  \n","   - Combines L1 and L2 regularization.  \n","\n","6. **SGDClassifier / SGDRegressor**  \n","   - Linear models optimized via **stochastic gradient descent**.  \n","\n","**Example: Linear Regression using `sklearn.linear_model`**  "],"metadata":{"id":"qQqdEBLgu7wE"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Sample data\n","X = np.array([[1], [2], [3], [4], [5]])\n","y = np.array([5, 7, 9, 11, 13])\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and train the model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = model.predict(X_test)\n","print(y_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gqdebQ7vI-U","executionInfo":{"status":"ok","timestamp":1761133505133,"user_tz":-330,"elapsed":5066,"user":{"displayName":"shivam singh","userId":"13846532010104603200"}},"outputId":"324b8086-ae5d-4cc6-f6d3-430ddcb1c232"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[7.]\n"]}]},{"cell_type":"markdown","source":["18. **What does model.fit() do? What arguments must be given?**\n","\n","`model.fit()` is the method used to **train a machine learning model** on the provided dataset. During fitting, the model **learns patterns from the input features (X) to predict the target variable (y)** by adjusting its internal parameters (weights and biases).  \n","\n","**In scikit-learn:**  \n","\n","**Basic Syntax:**  \n","```python\n","model.fit(X, y)\n","```\n","\n","Arguments:\n","\n","X → Input features (array-like, shape [n_samples, n_features])\n","\n","y → Target variable (array-like, shape [n_samples] for regression or classification)\n","\n","Optional parameters depending on the model:\n","\n","sample_weight → Array of weights for each sample (if needed)\n","\n","Example:\n","```python\n","from sklearn.linear_model import LinearRegression\n","import numpy as np\n","\n","X = np.array([[1], [2], [3], [4], [5]])\n","y = np.array([5, 7, 9, 11, 13])\n","\n","model = LinearRegression()\n","model.fit(X, y)  # Train the model\n","```\n","* After this, the model has learned the coefficients and intercept.\n","* You can now use model.predict(X_test) to make predictions.\n","```\n","model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n","```\n"],"metadata":{"id":"IaOXTpFbvNik"}},{"cell_type":"markdown","source":["19. **What does model.predict() do? What arguments must be given?**\n","\n","`model.predict()` is a method used to **make predictions using a trained machine learning model**. After the model has been trained using `model.fit()`, it can take new input data and output predicted values or classes.  \n","\n","**In scikit-learn:**  \n","\n","**Basic Syntax:**  \n","```python\n","predictions = model.predict(X_new)\n","```\n","Arguments:\n","\n","X_new → Input features for which predictions are to be made (array-like, shape [n_samples, n_features])\n","\n","Example:\n","```python\n","from sklearn.linear_model import LinearRegression\n","import numpy as np\n","\n","# Training data\n","X = np.array([[1], [2], [3], [4], [5]])\n","y = np.array([5, 7, 9, 11, 13])\n","\n","# Train the model\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","# New data for prediction\n","X_new = np.array([[6], [7]])\n","predictions = model.predict(X_new)\n","print(predictions)  # Output: predicted values for X_new\n","```\n","* Output will be continuous values for regression models or class labels for classification models."],"metadata":{"id":"KnNyur7TvYjM"}},{"cell_type":"markdown","source":["20. **What are continuous and categorical variables?**\n","\n","**1. Continuous Variables**  \n","- **Definition:** Variables that can take **any numerical value** within a range.  \n","- Often measured quantities that can be fractional or decimal.  \n","- **Examples:**  \n","  - Height (e.g., 170.5 cm)  \n","  - Weight (e.g., 65.2 kg)  \n","  - Temperature (e.g., 36.6°C)  \n","- **Use Case in ML:** Regression tasks, scaling and normalization may be applied.  \n","\n","---\n","\n","**2. Categorical Variables**  \n","- **Definition:** Variables that represent **discrete categories or labels**.  \n","- They do not have a natural numerical order (unless ordinal).  \n","- **Examples:**  \n","  - Gender (Male, Female)  \n","  - Color (Red, Blue, Green)  \n","  - Type of vehicle (Car, Bike, Bus)  \n","- **Use Case in ML:** Classification tasks; often encoded using techniques like **One-Hot Encoding** or **Label Encoding**.  \n","\n","**Key Difference:**  \n","| Feature Type      | Nature           | Values Example        | ML Use Case      |\n","|------------------|-----------------|---------------------|----------------|\n","| Continuous        | Numerical       | 0.5, 2.3, 100       | Regression, Scaling |\n","| Categorical       | Discrete/Labels | Red, Blue, Car       | Classification, Encoding |"],"metadata":{"id":"dgLwfyuAwcCc"}},{"cell_type":"markdown","source":["21. **What is feature scaling? How does it help in Machine Learning?**\n","\n","Feature scaling is the process of **normalizing or standardizing the range of independent variables (features)** in a dataset.  \n","It ensures that all features contribute equally to the learning process, preventing models from being biased toward features with larger numerical values.  \n","\n","Why Feature Scaling is Needed : In many Machine Learning algorithms, the distance between data points or the magnitude of feature values affects how the model learns.\n","For example, algorithms like K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Gradient Descent-based models are sensitive to differences in feature scales.\n","\n","If one feature (e.g., salary in thousands) has a much larger range than another (e.g., age in years), the model may give more importance to the higher-valued feature — even if it’s not more important.\n","\n","\n","**How Feature Scaling Helps in Machine Learning**\n","\n","* Improves Model Accuracy: Prevents large-scale features from dominating the model’s learning.\n","\n","* Faster Convergence: Algorithms using gradient descent (like Linear Regression or Neural Networks) converge faster when features are scaled.\n","\n","* Better Distance Calculations: Distance-based algorithms (KNN, K-Means, SVM) perform better when all features are on a similar scale.\n","\n","* Avoids Numerical Instability: Prevents computational issues when features have very large or very small values.\n"],"metadata":{"id":"utforgQeysB2"}},{"cell_type":"markdown","source":["22. **How do we perform scaling in Python?**\n","\n","Feature scaling in Python is commonly done using the **`scikit-learn`** library, which provides several preprocessing classes for different scaling techniques.\n","\n","\n","**1. Min-Max Scaling (Normalization)**  \n","This scales all values between a specific range (usually 0 to 1).\n","\n","```python\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Example data\n","data = [[10], [20], [30], [40], [50]]\n","\n","# Initialize scaler\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","# Fit and transform the data\n","scaled_data = scaler.fit_transform(data)\n","\n","print(scaled_data)\n","```\n","\n","**2. Standardization (Z-score Scaling)**\n","This method rescales data so that it has a mean of 0 and a standard deviation of 1.\n","```python\n","from sklearn.preprocessing import StandardScaler\n","\n","# Example data\n","data = [[10], [20], [30], [40], [50]]\n","\n","# Initialize scaler\n","scaler = StandardScaler()\n","\n","# Fit and transform the data\n","scaled_data = scaler.fit_transform(data)\n","\n","print(scaled_data)\n","```\n","\n","**3. Robust Scaling**\n","This scaling technique uses the median and interquartile range (IQR), making it robust to outliers.\n","```python\n","from sklearn.preprocessing import RobustScaler\n","\n","# Example data\n","data = [[10], [20], [3000], [40], [50]]\n","\n","# Initialize scaler\n","scaler = RobustScaler()\n","\n","# Fit and transform the data\n","scaled_data = scaler.fit_transform(data)\n","\n","print(scaled_data)\n","```\n","\n","**4. MaxAbs Scaling**\n","This scales each feature by dividing by its maximum absolute value, keeping the sign of the data intact.\n","```python\n","from sklearn.preprocessing import MaxAbsScaler\n","\n","# Example data\n","data = [[-10], [0], [10], [20]]\n","\n","# Initialize scaler\n","scaler = MaxAbsScaler()\n","\n","# Fit and transform the data\n","scaled_data = scaler.fit_transform(data)\n","\n","print(scaled_data)\n","```\n","\n","**5. Using fit(), transform(), and fit_transform()**\n","\n","* fit() — Learns the scaling parameters (mean, std, min, max, etc.) from the data.\n","\n","* transform() — Applies the learned scaling to the data.\n","\n","* fit_transform() — Performs both fit() and transform() in one step.\n","```python\n","scaler = StandardScaler()\n","scaler.fit(data)           # Learn parameters\n","scaled_data = scaler.transform(data)  # Apply scaling\n","```\n"],"metadata":{"id":"oktPPP1NySOc"}},{"cell_type":"markdown","source":["23. **What is sklearn.preprocessing?**\n","\n","The **`sklearn.preprocessing`** module in the **Scikit-learn** library provides various tools and techniques to **transform raw data into a suitable format** for Machine Learning models.  \n","It helps in **scaling, encoding, normalizing, and imputing** data so that models can learn effectively.\n","\n","**Key Functions and Classes in `sklearn.preprocessing`**\n","\n","1. **Scaling and Normalization**\n","   - `StandardScaler` – Standardizes features by removing the mean and scaling to unit variance.  \n","   - `MinMaxScaler` – Scales features to a specific range (default 0 to 1).  \n","   - `RobustScaler` – Uses median and IQR for scaling (robust to outliers).  \n","   - `MaxAbsScaler` – Scales each feature by its maximum absolute value.  \n","   - `Normalizer` – Normalizes samples individually to have unit norm (used in text and clustering).\n","\n","2. **Encoding Categorical Variables**\n","   - `LabelEncoder` – Encodes labels (target values) with integer values (0, 1, 2, …).  \n","   - `OneHotEncoder` – Converts categorical values into a one-hot numeric array.  \n","   - `OrdinalEncoder` – Encodes categorical features as integers with an assigned order.\n","\n","3. **Imputing Missing Values**\n","   - `SimpleImputer` – Replaces missing values with mean, median, or a constant.  \n","   - `KNNImputer` – Fills missing values using the mean value from nearest neighbors.\n","\n","4. **Generating Polynomial Features**\n","   - `PolynomialFeatures` – Expands input features into polynomial combinations to model nonlinear relationships.\n","\n","5. **Binarization and Thresholding**\n","   - `Binarizer` – Converts numerical features into binary (0/1) based on a threshold.\n","\n","**Example: Scaling and Encoding**\n","\n","```python\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","import numpy as np\n","\n","# Example numeric data\n","X_numeric = np.array([[10, 20, 30], [20, 30, 40], [30, 40, 50]])\n","\n","# Scaling\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X_numeric)\n","print(X_scaled)\n","\n","# Example categorical data\n","X_categorical = np.array([['red'], ['green'], ['blue']])\n","\n","# Encoding\n","encoder = OneHotEncoder()\n","X_encoded = encoder.fit_transform(X_categorical).toarray()\n","print(X_encoded)\n","```"],"metadata":{"id":"s8zYSvvTzTv3"}},{"cell_type":"markdown","source":["24. **How do we split data for model fitting (training and testing) in Python?**\n","\n","In Machine Learning, we split the dataset into **training** and **testing** sets to evaluate how well a model generalizes to unseen data.\n","\n","- **Training set** → Used to train (fit) the model.  \n","- **Testing set** → Used to evaluate the model’s performance.\n","\n","**Common Function Used**\n","\n","Scikit-learn provides the function **`train_test_split()`** from the `sklearn.model_selection` module to split data efficiently.\n","\n","**Syntax**\n","\n","```python\n","train_test_split(*arrays, test_size=0.25, train_size=None, random_state=None, shuffle=True)\n","```\n","\n","* arrays: The input data (features and target).\n","* test_size: Fraction or number of samples to include in the test split (e.g., 0.2 means 20%).\n","* random_state: Ensures reproducibility of results.\n","* shuffle: Whether to shuffle data before splitting (default: True)."],"metadata":{"id":"IZOpsD4S0pv3"}},{"cell_type":"markdown","source":["25. **Explain data encoding?**\n","\n","**Data Encoding**\n","\n","Data encoding is the process of **converting categorical or textual data into numerical form** so that machine learning algorithms can interpret and process it.  \n","Most ML algorithms (like regression, SVM, and neural networks) can only work with **numerical input**, not text or categories.\n","\n","---\n","\n","**Why Encoding is Important**\n","\n","1. Machine learning models require numerical data.  \n","2. Encoding converts labels or categories into numbers.  \n","3. It preserves useful information while making the data model-ready.\n","\n","---\n","**Data Encoding Techniques**\n","\n","**1. Label Encoding**  \n","Converts each category into a unique integer value.  \n","Used when categorical data is **ordinal** (has a natural order).\n","\n","---\n","\n","**2. One-Hot Encoding**  \n","Creates **binary columns (0 or 1)** for each category.  \n","Used when categorical data is **nominal** (no natural order).\n","\n","---\n","\n","**3. Ordinal Encoding**  \n","Assigns integer values to categories based on their **rank or order**.  \n","Commonly used for data with levels like \"Low\", \"Medium\", \"High\".\n","\n","---\n","\n","**4. Binary Encoding**  \n","Combines the properties of label and one-hot encoding.  \n","First converts categories to integers, then represents those integers as **binary digits**.  \n","Efficient for handling **high-cardinality categorical data**.\n","\n","---\n","\n","**5. Frequency Encoding**  \n","Replaces each category with the **frequency (count)** of its occurrence in the dataset.  \n","Useful when categorical data has many unique values.\n","\n","---\n","\n","**6. Target Encoding**  \n","Replaces each category with the **mean of the target variable** for that category.  \n","Common in supervised learning tasks like classification and regression.\n","\n","---\n","\n","**7. Hash Encoding (Feature Hashing)**  \n","Uses a **hash function** to convert categories into a fixed number of numeric columns.  \n","Efficient for datasets with a **large number of categories**.\n","\n","---\n","\n","**8. Count Encoding**  \n","Similar to frequency encoding but replaces categories with the **count** (not normalized frequency) of their occurrences.\n","\n","---\n","\n","**9. Helmert Encoding**  \n","Represents each category as a comparison of the mean of the previous categories.  \n","Used in some statistical modeling approaches.\n","\n","---\n","\n","**10. Leave-One-Out Encoding (LOO Encoding)**  \n","A variation of target encoding where the current row’s target is excluded from the mean calculation to avoid data leakage.\n","\n","---\n","\n","**11. Base-N Encoding**  \n","Encodes categories using **base-N representations** (e.g., base-2, base-3).  \n","It’s a generalization of binary encoding.\n","\n"],"metadata":{"id":"9vWzpCEZ1Vp6"}}]}